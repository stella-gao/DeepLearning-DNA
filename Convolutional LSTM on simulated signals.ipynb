{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.losses import categorical_crossentropy, kullback_leibler_divergence\n",
    "from keras.callbacks import TensorBoard\n",
    "%matplotlib inline\n",
    "from matplotlib import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = np.load('sample_data.npy')\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_signal_naive(my_sequence, length=250):\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    base_strength = [4, 3, 2, 1]\n",
    "    idx = my_sequence.argmax(1).reshape(my_sequence.shape[0],-1)\n",
    "    X = np.zeros((my_sequence.shape[0], 1, length, 1))\n",
    "    \n",
    "    for sample_no in range(my_sequence.shape[0]):\n",
    "        for ix in range(my_sequence.shape[2]):\n",
    "            base = idx[sample_no, ix]\n",
    "            X[sample_no, 0, ix*10:(ix*10+5), 0] +=  base_strength[base]\n",
    "    return X\n",
    "\n",
    "def generate_signal(my_sequence, length=250, channels=3):\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    base_strength = [[4, 3, 2, 1], [2, 3, 4, 1], [1, 4, 3, 2]]\n",
    "    base_noise = [.02, .01, .02, .01]\n",
    "    idx = my_sequence.argmax(1).reshape(my_sequence.shape[0],-1)\n",
    "    X = np.zeros((my_sequence.shape[0], channels, length, 1))\n",
    "    \n",
    "    for sample_no in range(my_sequence.shape[0]):\n",
    "        for ix in range(my_sequence.shape[2]):\n",
    "            base = idx[sample_no, ix]\n",
    "            for channel in range(channels):\n",
    "                mu = norm.rvs(loc=10, scale=2)\n",
    "                sigma = abs(norm.rvs(loc=10, scale=1))\n",
    "                amplitude = norm.rvs(loc=base_strength[channel][base], scale=base_noise[base])\n",
    "                X[sample_no, channel, :, 0] += amplitude*norm.pdf(np.arange(length), loc=mu+ix*10, scale= sigma)\n",
    "    return X\n",
    "\n",
    "def random_crop(template_dna, size=10):\n",
    "    ix = np.random.randint(0,template_dna.shape[2]-size, template_dna.shape[0])\n",
    "    cropped = np.zeros((template_dna.shape[0], template_dna.shape[1], size))\n",
    "    for i in range(len(ix)):\n",
    "        cropped[i] = template_dna[i, :, ix[i]:(ix[i]+size)].squeeze()\n",
    "    return cropped\n",
    "\n",
    "def get_batch(X_train, batch_size=50):\n",
    "    ix = -batch_size\n",
    "    while True:\n",
    "        if ix>=(X_train.shape[0]-batch_size):\n",
    "            ix = -batch_size\n",
    "        ix+=batch_size\n",
    "        template = X_train[ix:(ix+batch_size)]\n",
    "       \n",
    "        dna = random_crop(template, size=10)\n",
    "        signal = generate_signal(dna, length=100)\n",
    "        \n",
    "        yield  template, dna, signal\n",
    "\n",
    "        \n",
    "# Spare some data for validation\n",
    "template = X_train[:500]\n",
    "val_dna = random_crop(template, size=10)\n",
    "val_signal = generate_signal(val_dna, length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = random_crop(X_train[:2,:,:,:], size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Rerun this code to see how the same DNA seq can give very different profiles\n",
    "x_ = generate_signal(x, length=100)\n",
    "pl.plot(x_[0,0,:,0])\n",
    "pl.plot(x_[0,1,:,0])\n",
    "pl.plot(x_[0,2,:,0])\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvLSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 32 \n",
    "#meta timesteps\n",
    "n_steps = 10 # timesteps \n",
    "n_hidden = 50 # hidden layer num of features\n",
    "n_classes = 4 # bases\n",
    "\n",
    "#predict all bases from the signal or just the leftmost base\n",
    "seq_pred = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## input signal with 3 features, 100 timepoints\n",
    "inp = tf.placeholder(tf.float32, [None, 3, 100, 1])\n",
    "if seq_pred:\n",
    "    targets = tf.placeholder(tf.float32, [None, n_classes, n_steps])\n",
    "    list_targets = tf.unstack(targets, n_steps, 2)\n",
    "else:\n",
    "    targets = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    \n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def RNN(x, weights, biases, return_seq=False):\n",
    "\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    # converted shape:'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    if return_seq:\n",
    "        return [tf.matmul(outp, weights['out']) + biases['out'] for outp in outputs]\n",
    "    else:\n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Conv2D(32, [3, 20],\n",
    "                padding='same',\n",
    "                name='conv_1')(inp)\n",
    "net = tf.reduce_sum(net, axis=1, keep_dims=True)\n",
    "net = MaxPooling2D((1, 10), strides=(1, 10))(net)\n",
    "net = tf.squeeze(net, squeeze_dims=1)\n",
    "\n",
    "preds = RNN(net, weights, biases, return_seq=seq_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if seq_pred:\n",
    "    losses = [tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=target) \n",
    "              for pred,target in zip(preds, list_targets)]\n",
    "    correct_pred = [tf.equal(tf.argmax(pred,1), tf.argmax(target,1)) \n",
    "                    for pred,target in zip(preds, list_targets)]\n",
    "else:\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=targets)\n",
    "    correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(targets,1))\n",
    "    \n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(losses)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For tensorboard viz\n",
    "\n",
    "\n",
    "if seq_pred:\n",
    "    run_name = 'convLstm_seq'\n",
    "else:\n",
    "     run_name = 'convLstm'\n",
    "\n",
    "tf.summary.scalar('classification_cost', cost)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "summary_writer_train = tf.summary.FileWriter( run_name+'/train', sess.graph)\n",
    "summary_writer_valid = tf.summary.FileWriter( run_name+'/validation', sess.graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from terminal type:\n",
    "# tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batcher = get_batch(X_train[500:], batch_size=batch_size)\n",
    "step=0\n",
    "for i in range(training_iters):\n",
    "    for j in range(10):\n",
    "        _, dna, signal = batcher.next()\n",
    "        if seq_pred:\n",
    "            y_target = dna #predict all 10 bases\n",
    "            y_valid = val_dna\n",
    "        else:\n",
    "            y_target = dna[:,:,0] # predict only the leftmost base\n",
    "            y_valid = val_dna[:,:,0]\n",
    "            \n",
    "        _ = sess.run(optimizer, feed_dict={inp:signal, targets:y_target, K.learning_phase():1})\n",
    "    \n",
    "    np_acc_tr, np_loss_tr, train_summary = sess.run([accuracy, cost, summary_op], \n",
    "                                  feed_dict={inp:signal, targets: y_target, K.learning_phase():0})\n",
    "    np_acc_vl, np_loss_vl, validation_summary = sess.run([accuracy, cost,  summary_op], \n",
    "                                  feed_dict={inp:val_signal, targets: y_valid, K.learning_phase():0})\n",
    "    print('Iteration {}\\t train_loss:{:.4f}\\t val_loss:{:.4f}\\t  \\\n",
    "    train_acc:{:.2f}\\t val_acc:{:.2f}\\t'.format(i, np_loss_tr, np_loss_vl, np_acc_tr, np_acc_vl))\n",
    "    summary_writer_train.add_summary(train_summary, step)\n",
    "    summary_writer_valid.add_summary(validation_summary, step)\n",
    "    summary_writer_train.flush()\n",
    "    summary_writer_valid.flush()\n",
    "    step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
